{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Image Classification with Deep Learning\n",
    "\n",
    "Matthias Griebel<br>\n",
    "Chair of Information Systems and Management\n",
    "\n",
    "Winter Semester 20/21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-vcxRl34g6d",
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Capstone-Project\" data-toc-modified-id=\"Capstone-Project-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Capstone Project</a></span><ul class=\"toc-item\"><li><span><a href=\"#iWildCam-2020\" data-toc-modified-id=\"iWildCam-2020-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>iWildCam 2020</a></span></li><li><span><a href=\"#Project-Organization\" data-toc-modified-id=\"Project-Organization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Project Organization</a></span></li></ul></li><li><span><a href=\"#Processing-Image-Data\" data-toc-modified-id=\"Processing-Image-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Processing Image Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Requirements</a></span></li><li><span><a href=\"#Dataset:-Imagewoof\" data-toc-modified-id=\"Dataset:-Imagewoof-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Dataset: Imagewoof</a></span></li><li><span><a href=\"#Looking-at-the-data\" data-toc-modified-id=\"Looking-at-the-data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Looking at the data</a></span></li><li><span><a href=\"#Creating-DataLoaders\" data-toc-modified-id=\"Creating-DataLoaders-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Creating DataLoaders</a></span><ul class=\"toc-item\"><li><span><a href=\"#Factory-Methods:-ImageDataLoaders\" data-toc-modified-id=\"Factory-Methods:-ImageDataLoaders-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Factory Methods: ImageDataLoaders</a></span></li></ul></li><li><span><a href=\"#The-data-block-API\" data-toc-modified-id=\"The-data-block-API-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>The data block API</a></span></li></ul></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Training</a></span><ul class=\"toc-item\"><li><span><a href=\"#The-cnn_learner\" data-toc-modified-id=\"The-cnn_learner-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>The cnn_learner</a></span></li><li><span><a href=\"#Find-the-learning-rate\" data-toc-modified-id=\"Find-the-learning-rate-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Find the learning rate</a></span></li><li><span><a href=\"#Fit-the-model\" data-toc-modified-id=\"Fit-the-model-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Fit the model</a></span></li><li><span><a href=\"#Unfreeze-and-train-again\" data-toc-modified-id=\"Unfreeze-and-train-again-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Unfreeze and train again</a></span></li></ul></li><li><span><a href=\"#Results\" data-toc-modified-id=\"Results-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Top-Losses\" data-toc-modified-id=\"Top-Losses-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Top Losses</a></span></li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Confusion Matrix</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIgASMCH4g6e",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "Materials taken from\n",
    "- https://github.com/fastai/course-v3/blob/master/nbs/dl1/lesson1-pets.ipynb\n",
    "- https://github.com/hiromis/notes/blob/master/Lesson1.md\n",
    "- https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItvnrF3H4g6f",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHDnaP6Y4g6g",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### iWildCam 2020\n",
    "\n",
    "From [Kaggle, 2020](https://www.kaggle.com/c/iwildcam-2020-fgvc7/overview)\n",
    "\n",
    "Camera Traps (or Wild Cams) enable the automatic collection of large quantities of image data. Biologists all over the world use camera traps to monitor biodiversity and population density of animal species. We have recently been making strides towards automatic species classification in camera trap images. However, as we try to expand the scope of these models we are faced with an interesting problem: how do we train models that perform well on new (unseen during training) camera trap locations? Can we leverage data from other modalities, such as citizen science data and remote sensing data?\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/5734e9927f990b2d6df85659688d501fc4872884bcd227933ba7224d9f59dcb0/68747470733a2f2f7261776769742e636f6d2f7669736970656469612f6977696c6463616d5f636f6d702f323032302f6173736574732f6977696c6463616d323032306865616465722e6a7067\" style=\"width:70%\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ps02SLhw4g6j",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Data set__\n",
    "\n",
    "The WCS training set contains 217,959 images from 441 locations, and the WCS test set contains 62,894 images from 111 locations. These 552 locations are spread across the globe.\n",
    "\n",
    "See [Kaggle Dataset Description](https://www.kaggle.com/c/iwildcam-2020-fgvc7/data).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxEqEFpk4g6i",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Objective__\n",
    "\n",
    "Develop deep learning models to automatically detect and classify species by means of \n",
    "\n",
    "1. Image Classification \n",
    "2. Object Detection with Bounding Boxes or Segmentation Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qH6Nqzag4g6m",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Project Organization\n",
    "\n",
    "- Deadline: tbd. (expected: end of February 2021)\n",
    "- Submission as Github Repository containing at least one Jupyter Notebook Document\n",
    "- Project Goals\n",
    "    - Basic Image Classification \n",
    "    - Object Detection with Bounding Boxes or Segmentation Masks\n",
    "    - Submissions to Kaggle (Score Reporting)\n",
    "- Form and structure should be similar to seminar papers or theses\n",
    "    - Containing explanations, figures, tables, and code\n",
    "    - Avoid large code blocks by modularization/outsourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3ubVyqE4g6n",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Notes__\n",
    "\n",
    "- Please assign yourself to a github group repository (link on wuecampus)\n",
    "- You will need a Kaggle Account to access the data and computing resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNtRTLxq4g6o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Processing Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ld9hU0rq4g6o",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "__Hardware: Graphics Processing Unit (GPU)__\n",
    "\n",
    "GPU is fit for training the deep learning systems in a long run for very large datasets. CPU can train a deep learning model quite slowly. GPU accelerates the training of the model. Hence, GPU is a better choice to train the Deep Learning Model efficiently and effectively ([Medium](https://medium.com/@shachishah.ce/do-we-really-need-gpu-for-deep-learning-47042c02efe2)).\n",
    "\n",
    "Make sure your GPU environment is set up and you can run Jupyter Notebook.\n",
    "\n",
    "__GPU on [Google Colab](http://colab.research.google.com)__\n",
    "\n",
    "* Select 'Runtime' -> 'Change runtime time' -> 'Python 3' (and 'GPU') before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EL0p7qe4g6p",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Libraries__\n",
    "\n",
    "We are going to work with the fastai V2 library which sits on top of Pytorch.\n",
    "The fastai library as a layered API as summarized by this graph:\n",
    "\n",
    "<img src=\"https://docs.fast.ai/images/layered.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to install/upgrade fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxUwshbf4g6s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq fastai  # upgrade fastai on colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UxUwshbf4g6s"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SZ-q11uu4g6z",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataset: Imagewoof\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/960/1*zZJpK1EXPU-gVyt46kNypQ.jpeg\" width=\"500\" align=\"right\"/>\n",
    "\n",
    "\n",
    "We are going to use the [Imagewoof](https://github.com/fastai/imagenette) data set, a subset of 10 classes from Imagenet that aren't so easy to classify, since they're all dog breeds. \n",
    "\n",
    "The breeds are: Australian terrier, Border terrier, Samoyed, Beagle, Shih-Tzu, English foxhound, Rhodesian ridgeback, Dingo, Golden retriever, Old English sheepdog. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IjFiRmsR4g60",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Download and extract__\n",
    "\n",
    "The first thing we have to do is download and extract the data that we want. `untar_data` will download that to some convenient path and untar it for us and it will then return the value of path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uuV7irXR4g61",
    "outputId": "985cd69c-2551-4e45-f26a-97ac7fefa251"
   },
   "outputs": [],
   "source": [
    "# URLs.IMAGEWOOF_160 = 'https://s3.amazonaws.com/fast-ai-imageclas/imagewoof-160'\n",
    "path = untar_data(URLs.IMAGEWOOF_160); path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRPi1lwb4g64"
   },
   "source": [
    "Next time you run this, since you've already downloaded it, it won't download it again. Since you've already untared it, it won't untar it again. So everything is designed to be pretty automatic and easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1c-iEEw4g65",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Looking at the data\n",
    "\n",
    "The first thing we do when we approach a problem is to take a look at the data. We always need to understand very well what the problem is and what the data looks like before we can figure out how to solve it. Taking a look at the data means understanding how the data directories are structured, what the labels are and what some sample images look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huhhUnzR4g66",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Python 3 pathlib__\n",
    "\n",
    "For convenience, fast.ai adds functionality into existing Python stuff. One of these things is add a `ls()` method to path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "aQfrkYyT4g66",
    "outputId": "37f4ac69-852d-495c-c846-fe9114b8f3da"
   },
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wUq7SzoZ4g69",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Path objects from the [pathlib](https://docs.python.org/3/library/pathlib.html) module are much better to use than strings. It doesn't matter if you're on Windows, Linux, or Mac. It is always going to work exactly the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j44eTNB24g7G",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__get_image_files__\n",
    "\n",
    "`get_image_files` will just grab an array of all of the image files based on extension in a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "1Alu06Dr4g7H",
    "outputId": "e2d2bdc3-9375-4ef8-8d35-0d5d6111e7ec"
   },
   "outputs": [],
   "source": [
    "fnames = get_image_files(path/'train/n02115641')\n",
    "fnames[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Niy9jFxS4g7M"
   },
   "source": [
    "Here, 'n02115641' refers to the class _dingo_ in [imagenet](https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5p5HsAc4g7M",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Creating DataLoaders\n",
    "\n",
    "The main difference between the handling of image classification datasets is the way labels are stored. In this particular dataset, labels are stored in the names of the (sub-)folders. We will need to extract them to be able to classify the images into the correct categories. \n",
    "\n",
    "We will now explore different ways load these such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Factory Methods: ImageDataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`ImageDataLoaders`](https://docs.fast.ai/vision.data.html#ImageDataLoaders) Class is a basic wrapper around several DataLoaders with factory methods for computer vision problems.\n",
    "\n",
    "Here, we can use `ImageDataLoaders.from_folder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(path, train='train', valid='val', item_tfms=Resize(224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sPF_aN5Z4g7i",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__data.show_batch__\n",
    "\n",
    "Let's take a look at a few pictures. `dls.show_batch` can be used to show me some of the contents  So you can see roughly what's happened is that they all seem to have being zoomed and cropped in a reasonably nice way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hf_KX8E44g7X",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The data block API\n",
    "\n",
    "The [data block API](https://docs.fast.ai/data.block.html#DataBlock) lets you customize the creation of the Dataloaders by isolating the underlying parts of that process in separate blocks, mainly:\n",
    "\n",
    "1. The types of your input and labels\n",
    "2. `get_items` (how to get your input)\n",
    "3. `splitter` (How to split the data into a training and validation sets?)\n",
    "4. `get_y` (How to label the inputs?)\n",
    "\n",
    "... and suitable transforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5aDEM78S4g7N"
   },
   "outputs": [],
   "source": [
    "woof = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files, \n",
    "                 splitter=GrandparentSplitter(train_name='train', valid_name='val'),\n",
    "                 get_y=parent_label,\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One important piece of this `DataBlock` call that we haven't seen before is in these two lines:\n",
    "\n",
    "```python\n",
    "item_tfms=Resize(460),\n",
    "batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
    "```\n",
    "\n",
    "These lines implement a fastai data augmentation strategy which we call *presizing*. Presizing is a particular way to do image augmentation that is designed to minimize data destruction while maintaining good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Presizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Presizing on the training set\" width=\"600\" caption=\"Presizing on the training set\" id=\"presizing\" src=\"https://raw.githubusercontent.com/fastai/fastbook/master/images/att_00060.png\" align=\"right\">\n",
    "\n",
    "Presizing adopts two strategies\n",
    "\n",
    "1. *Crop full width or height*: This is in `item_tfms`, so it's applied to each individual image before it is copied to the GPU. It's used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.\n",
    "2. *Random crop and augment*: This is in `batch_tfms`, so it's applied to a batch all at once on the GPU, which means it's fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentations are done first.\n",
    "\n",
    "To implement this process in fastai you use `Resize` as an item transform with a large size, and `RandomResizedCrop` as a batch transform with a smaller size. `RandomResizedCrop` will be added for you if you include the `min_scale` parameter in your `aug_transforms` function, as was done in the `DataBlock` call in the previous section. Alternatively, you can use `pad` or `squish` instead of `crop` (the default) for the initial `Resize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmZ8BqQ34g7Y"
   },
   "source": [
    "From the Datablock we can automatically get a our DataLoaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = woof.dataloaders(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and have a look at the summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woof.summary(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t4NbvibI4g7Y",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To make the classes easier to read and interpret, we can modify our `get_y` function. First, we define a dictionary for the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g82_t0Y4g7Z"
   },
   "outputs": [],
   "source": [
    "dogs_dict = { 'n02086240': 'Shih-Tzu',\n",
    "              'n02087394': 'Rhodesian_ridgeback',\n",
    "              'n02088364': 'beagle',\n",
    "              'n02089973': 'English_foxhound',\n",
    "              'n02093754': 'Border_terrier',\n",
    "              'n02096294': 'Australian_terrier',\n",
    "              'n02099601': 'golden_retriever',\n",
    "              'n02105641': 'Old_English_sheepdog',\n",
    "              'n02111889': 'Samoyed',\n",
    "              'n02115641': 'dingo',\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define our own `get_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_y = lambda x: dogs_dict[x.parent.name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KzDG6bM4g7b",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, we can create the `DataBlock` again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woof = DataBlock(blocks = (ImageBlock, CategoryBlock),\n",
    "                 get_items=get_image_files, \n",
    "                 splitter=GrandparentSplitter(train_name='train', valid_name='val'),\n",
    "                 get_y=get_y,\n",
    "                 item_tfms=Resize(460),\n",
    "                 batch_tfms=aug_transforms(size=224, min_scale=0.75)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = woof.dataloaders(path)\n",
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s_z3DDt14g7n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3S0H3VO4g7p"
   },
   "source": [
    "Now we will start training our model. We will use a convolutional neural network backbone and a fully connected head with a single hidden layer as a classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvfdJuVC4g7p",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The cnn_learner\n",
    "\n",
    "This method creates a Learner object from the data object and model inferred from it with the backbone given in `arch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9BTqL7yg4g7q",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, models.resnet34, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dfbmKODJ4g7s"
   },
   "source": [
    "- __dls__: Dataloaders\n",
    "- __arch__: architecture. There are lots of different ways of constructing a convolutional neural network. For now, the most important thing for you to know is that there's a particular kind of model called ResNet which works extremely well nearly all the time. For a while, at least, you really only need to be doing choosing between two things which is what size ResNet do you want. There are ResNet34 and ResNet50. \n",
    "- __metrics__: error rate (1-accuracy)\n",
    "- __loss_func__: automatically inferred from `dls`. What kind of loss function would typically choose for this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dbu8MCUb4g7t",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's print a summary of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nV6H9b7f4g7t",
    "outputId": "ea8938b4-bb99-449a-ec94-a96158331a34"
   },
   "outputs": [],
   "source": [
    "# Open issue at current fastai version: https://github.com/fastai/fastai/issues/3011\n",
    "# learn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SnqMVrQo4g7v",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Resnet Architecture__\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1314/1*S3TlG0XpQZSIpoDIUCQ0RQ.jpeg\" style=\"width:70%\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXBTdjKI4g7w",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Find the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUttjTvSaTzX"
   },
   "source": [
    "Please read the fast.ai [lr_finder docs](https://docs.fast.ai/callback.schedule#Learner.lr_find). Also, Sylvain Gugger from the fast.ai team wrote a nice [blog post](https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html) on how to find  good learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tavTRz7x4g7w",
    "outputId": "19ab441c-1232-41c7-d644-d5cb4e49af62"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZtV3eXF4g72",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fit the model\n",
    "\n",
    "Fit the model based on selected learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "DRDLUuRe4g72",
    "outputId": "397a4043-60e8-4c14-8696-c87207250d7a"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, lr_max=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MVOBZHDQ4g75"
   },
   "outputs": [],
   "source": [
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBho1XVmbdin"
   },
   "source": [
    "So far we have fitted 2 epochs and it ran pretty quickly. Why is that so? Because we used a little trick (called transfer learning).\n",
    "\n",
    "What did we do?\n",
    "We added a few extra layers at the end of architecture, and we only trained those. We left most of the early layers as they were. This is called freezing layers i.e weights of the layers.\n",
    "\n",
    "- When we call fit or `fit_one_cycle()` on a create_cnn, it will just fine-tune these extra layers at the end, and run very fast.\n",
    "- To get a better model, we have to call `unfreeze()` to train the whole model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YDA_oIW4g78",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Unfreeze and train again\n",
    "\n",
    "Since our model is working as we expect it to, we will unfreeze our model and train some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5uTKDT04g79"
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vKINuvlY4g7_",
    "outputId": "501e5b2b-193f-4071-9193-d3ec00f30d2f"
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeRf6bCs6rY6",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Learning rates after unfreezing__\n",
    "\n",
    "The basic rule of thumb is after you unfreeze (i.e. train the whole thing), pass a max learning rate parameter, pass it a slice, make the second part of that slice about 10 times smaller than your first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "o1OPJCKP4g8H",
    "outputId": "a4cf4695-1ac9-4599-8a47-d6aab6e44446"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4, lr_max=slice(1e-6,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_fUTZdUX4g8M"
   },
   "source": [
    "If the model overfits we can reload stage 1 and train the model again for fewer epochs or another learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QxGWQBt4g8N"
   },
   "outputs": [],
   "source": [
    "learn = learn.load('stage-1')\n",
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "G5vtSxstZW-j",
    "outputId": "689d88af-b2b1-4411-a3d1-9d81fa707572"
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lr_max=slice(1e-6,1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6RSRMXe4g8P",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kzqHFBvPc7fo"
   },
   "source": [
    "It’s important to see what comes out of our model. We have seen one way of what goes in, now let’s see what our model has predicted.\n",
    "\n",
    "The `ClassificationInterpretation` class has methods for creating confusion matrix as well as plotting misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0Wti_ljc4g8Q",
    "outputId": "33688dd4-0436-4ff9-fac2-174984dc6fe1"
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DHDHLEa4g8P",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Top Losses\n",
    "\n",
    "We will first see which were the categories that the model most confused with one another. We will try to see if what the model predicted was reasonable or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "colab_type": "code",
    "id": "6UqiX5NgQl9r",
    "outputId": "2da90610-f630-4b57-84e1-eca9627e9e57"
   },
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfD5VK8d4g8T",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Furthermore, when we plot the confusion matrix. Interestingly, the model often confuses English Foxhounds with Beagles. This confirmes that our model works very well until a certain level as these two dog breeds look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "id": "5XFcBqEE4g8T",
    "outputId": "855e78d4-067d-442d-b040-30b701c7af82"
   },
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roFnT8fd4g8W",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Most Confused__\n",
    "\n",
    "Sorted descending list of largest non-diagonal entries of confusion matrix, presented as actual, predicted, number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "gdqVti_y4g8Y",
    "outputId": "b672692f-43e6-4c41-a74b-b43b6179464d"
   },
   "outputs": [],
   "source": [
    "interp.most_confused(min_val=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "rise": {
   "enable_chalkboard": true,
   "overlay": "<div class='background'></div><div class='header'>WS 20/21</br>PDS</div><div class='logo'><img src='images/unilogo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
