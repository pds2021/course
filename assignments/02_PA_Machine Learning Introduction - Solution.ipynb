{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Practical Data Science 19/20*\n",
    "# Programming Assignment 2 - Predicting Video Game Sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment you need to apply your new (or refreshed) machine learning knowledge. You will need to create a modeling pipeline training and evaluating a machine learning model build on several numeric as well as categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Dataset\n",
    "\n",
    "You are provided with a dataset containing a list of video games with sales greater than 100.000 copies. Your task is to build a model predicting the yearly global sales (column ``Global_Sales``) of a video game leveraging the available features.\n",
    "\n",
    "To help you get started, the following blocks of code import the dataset using pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'https://github.com/pds2021/course/raw/main/assignments/Data/02/video_game_sales.csv'\n",
    "game_sales_data = pd.read_csv(data_path)\n",
    "game_sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Dataset\n",
    "\n",
    "Before you can get started training a machine learning model you will have to split the dataframe into features and the target variable (try to use as many features as possible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_sales_data.set_index('Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_sales_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = game_sales_data['Global_Sales']\n",
    "X = game_sales_data.drop('Global_Sales', axis=1)\n",
    "print(y.head())\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will have to create a train-test split in order to be able to evaluate your models. Use 80\\% of the data for training and 20\\% for evaluation (take a look at the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to identify the relevant parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, \n",
    "                                                  train_size=0.8, \n",
    "                                                  random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing missing values\n",
    "If you inspect your training data you will find that some of the variables have missing values. Use the ``SimpleImputer`` to replace missing values in numerical columns with the column mean and missing values in categorical columns with the most frequent value (take a look at the SimpleImputer [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) to identify the relevant parameters). You can decide if you want to use the simple or the advanced imputation strategy (or just try both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [col for col in train_X.columns if train_X[col].dtype == 'float64']\n",
    "cat_cols = [col for col in train_X.columns if train_X[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "train_X_num_imputed = pd.DataFrame(num_imputer.fit_transform(train_X[num_cols]), \n",
    "                                   columns=num_cols, index=train_X.index)\n",
    "val_X_num_imputed = pd.DataFrame(num_imputer.transform(val_X[num_cols]), \n",
    "                                   columns=num_cols, index=val_X.index)\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train_X_cat_imputed = pd.DataFrame(cat_imputer.fit_transform(train_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=train_X.index)\n",
    "val_X_cat_imputed = pd.DataFrame(cat_imputer.transform(val_X[cat_cols]), \n",
    "                                   columns=cat_cols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical variables\n",
    "\n",
    "Prior to training your model you will have to encode the categorical variables. Inspect all categorical variables and use the ``LabelEncoder`` or the ``OneHotEncoder`` where appropriate. Remember that you have to combine the numerical as well as the label encoded and the one hot encoded dataframes at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_cols:\n",
    "    print(\"{}: {}\".format(cat, game_sales_data[cat].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_sales_data['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "train_X_cat_label = pd.DataFrame(ordinal_encoder.fit_transform(train_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=train_X_cat_imputed.index)\n",
    "val_X_cat_label = pd.DataFrame(ordinal_encoder.transform(val_X_cat_imputed[[\"Platform\", 'Genre']]),\n",
    "                                 columns=[\"Platform\", 'Genre'], \n",
    "                                 index=val_X_cat_imputed.index)\n",
    "\n",
    "\n",
    "ohe_encoder = OneHotEncoder(sparse=False)\n",
    "train_X_cat_ohe = pd.DataFrame(ohe_encoder.fit_transform(train_X_cat_imputed[['Rating']]),\n",
    "                                 index=train_X_cat_imputed.index)\n",
    "val_X_cat_ohe = pd.DataFrame(ohe_encoder.transform(val_X_cat_imputed[['Rating']]),\n",
    "                                 index=val_X_cat_imputed.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.concat([train_X_num_imputed, train_X_cat_label, train_X_cat_ohe], axis=1)\n",
    "val_X = pd.concat([val_X_num_imputed, val_X_cat_label, val_X_cat_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now our dataset should be ready and we can train a predictive model. Train a Decision Tree as well as a Random Forest and compare the in-sample as well as the out-of-sample performance of both models usinge the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model_rf = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "    model_rf.fit(X_train, y_train)\n",
    "    preds_rf = model_rf.predict(X_valid)\n",
    "    model_dt = DecisionTreeRegressor(random_state=1)\n",
    "    model_dt.fit(X_train, y_train)\n",
    "    preds_dt = model_dt.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds_rf), mean_absolute_error(y_valid, preds_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_rf, oos_dt = score_dataset(train_X, val_X, train_y, val_y)\n",
    "is_rf, is_dt = score_dataset(train_X, train_X, train_y, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out-of-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(oos_rf, oos_dt))\n",
    "print('------------------------------')\n",
    "print('In-sample\\nRandom Forest: {}\\nDecicion Tree\" {}'.format(is_rf, is_dt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
